\chapter*{List of Symbols}
\addcontentsline{toc}{chapter}{List of Symbols}

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{p{3cm} p{10cm}}
\textbf{Symbol} & \textbf{Description} \\
\hline
\endfirsthead
\textbf{Symbol} & \textbf{Description} \\
\hline
\endhead
\hline
\multicolumn{2}{r}{\textit{Continued on next page}} \\
\endfoot
\endlastfoot

\multicolumn{2}{l}{\textit{Spaces and Sets}} \\
$\mathbb{N}$ & Set of natural numbers \\
$\mathbb{R}$ & Set of real numbers \\
$\mathbb{C}$ & Set of complex numbers \\
$\mathbb{R}^n$ & $n$-dimensional Euclidean space \\
$\mathcal{U}, \mathcal{V}$ & Banach or Hilbert spaces (typically domain and codomain) \\
$L^p(\Omega)$ & Lebesgue space of measurable functions with $p$-integrable absolute value ($1 \le p < \infty$) or essentially bounded ($p=\infty$) \\
$C^k(\Omega)$ & Space of $k$-times continuously differentiable functions \\
$C^\infty_0(\mathbb{R}^n)$ & Space of smooth functions with compact support \\
$\mathcal{D}(A)$ & Domain of an operator or functional $A$ \\
$\mathcal{R}(K)$ & Range of operator $K$ \\
$\mathcal{N}(K)$ & Null space (kernel) of operator $K$ \\
\hline
\multicolumn{2}{l}{\textit{Operators and Functions}} \\
$K$ & Forward operator (linear and bounded unless stated otherwise) \\
$K^*$ & Adjoint operator \\
$K^\dagger$ & Moore-Penrose generalised inverse \\
$I$ & Identity operator \\
$\mathcal{F}, \mathcal{F}^{-1}$ & Fourier transform and its inverse \\
$\mathcal{R}$ & Radon transform \\
$\partial F(u)$ & Subdifferential of a convex functional $F$ at $u$ \\
$\nabla F(u)$ & Gradient of a differentiable functional $F$ at $u$ \\
$\operatorname{prox}_{\tau J}$ & Proximal operator of $J$ with parameter $\tau$ \\
$\text{Lip}_1(\mathcal{U})$ & Space of 1-Lipschitz functions on $\mathcal{U}$ \\
\hline
\multicolumn{2}{l}{\textit{Deep Learning Components}} \\
$\sigma, \varphi$ & Activation functions (sigmoid, ReLU, softmax, etc.) \\
$H$ & Heaviside step function \\
$W$ & Weight matrix in neural network layer \\
$b$ & Bias vector in neural network layer \\
$\Theta, \Phi, \theta$ & Parameters of neural networks (encoders, decoders, transforms) \\
$f_w, f_\theta$ & General neural network function with parameters $w$ or $\theta$ \\
$E_\Theta$ & Encoder neural network with parameters $\Theta$ \\
$D_\Phi$ & Decoder neural network with parameters $\Phi$ \\
$T_\theta$ & Transform/Null-space network with parameters $\theta$ \\
$C_\Theta$ & Critic network (adversarial setting) with parameters $\Theta$ \\
$\mathcal{H}, \mathcal{F}$ & Learnable or residual mappings in neural network blocks \\
\hline
\multicolumn{2}{l}{\textit{Spaces and Feature Representations}} \\
$\mathcal{Z}$ & Feature space or latent space \\
$\gamma$ & Latent code (element of $\mathcal{Z}$) \\
\hline
\multicolumn{2}{l}{\textit{Probability and Expectations}} \\
$\mathbb{E}[\cdot]$ & Expectation operator \\
$\mathbb{P}$ & Probability measure or distribution \\
$\pi_u$ & Prior distribution on unknown $u$ \\
$\mathbb{P}_r$ & Distribution of real/clean data \\
$\mathbb{P}_{rec}$ & Distribution of naive reconstructions \\
\hline
\multicolumn{2}{l}{\textit{Signal and Noise Characterisation}} \\
$\Pi_j$ & Signal power of $j$-th spectral component \\
$\Delta_j$ & Noise power of $j$-th spectral component \\
$\Sigma$ & Covariance operator of the signal \\
\hline
\multicolumn{2}{l}{\textit{Distances and Divergences}} \\
$D_J^{abs}(u,v)$ & Absolute Bregman distance between $u$ and $v$ \\
$W_1$ & Wasserstein-1 distance (Earth Mover's Distance) \\
\hline
\multicolumn{2}{l}{\textit{Quantities and Parameters}} \\
$u$ & Unknown physical quantity (solution) \\
$u^\dagger$ & True or selected solution (e.g. generalised inverse solution) \\
$f$ & Measurement data \\
$f^\delta$ & Noisy measurement data \\
$\delta$ & Noise level (upper bound on $\|f - f^\delta\|$) \\
$\alpha$ & Regularisation parameter \\
$\sigma_j$ & Singular values \\
$\lambda_i$ & Eigenvalues \\
\hline
\multicolumn{2}{l}{\textit{Functionals and Measures}} \\
$\|\cdot\|$ & Norm (subscript denotes space, e.g., $\|\cdot\|_\mathcal{U}$) \\
$\langle \cdot, \cdot \rangle$ & Inner product \\
$F(Ku, f)$ & Data fidelity term \\
$J(u)$ & Regularisation functional \\
$\Phi(u)$ & Tikhonov functional (or general objective function) \\
$D_J^p(u, v)$ & Bregman distance generated by $J$ and subgradient $p$ \\
$\operatorname{TV}(u)$ & Total Variation \\
\end{longtable}